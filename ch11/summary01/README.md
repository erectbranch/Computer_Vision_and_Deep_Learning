# 11 Vision transformer

- Transformer ì›ë¦¬ì™€ ì „ê°œ ê³¼ì •ì´ ì˜ ì •ë¦¬ëœ ì„œë² ì´ ë…¼ë¬¸

    > [A survey of Transformers](https://arxiv.org/abs/2106.04554)

    > [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)

- Transformerì— CVë¥¼ ì ìš©í•œ ì„œë² ì´ ë…¼ë¬¸

    > [Transformers in Vision: A Survey](https://arxiv.org/abs/2101.01169)

    > [A Survey on Vision Transformer](https://arxiv.org/abs/2012.12556)

---

- ì‚¬ëŒì€ ì˜ìƒì„ ì¸ì‹í•  ë•Œ intention(ì˜ë„)ì— ë”°ë¼ íŠ¹ì • í¬ì¸íŠ¸ë¥¼ **attention**(ì£¼ëª©)í•œë‹¤.

- **VQA**(Visual Question Answering): ì˜ìƒê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µì„ í•˜ëŠ” Computer Vision

  - ì´ë¥¼ ìœ„í•´ì„œëŠ” ì‚¬ëŒì˜ attentionê³¼ ê°™ì€ ëŠ¥ë ¥ì„ ê°–ì¶°ì•¼ í•œë‹¤.

    > ìµœê·¼ attention mechanismë§Œìœ¼ë¡œ êµ¬ì„±ëœ transformer modelì´ CNN ì„±ëŠ¥ì„ ë›°ì–´ë„˜ëŠ”ë‹¤.

---

### 11.1.1 ê³ ì „ ì•Œê³ ë¦¬ì¦˜: feature selection

> [image recognization ë¬¸ì œì—ì„œì˜ ì£¼ìš” í•´ì„ ë°©ë²•](https://www.cognex.com/ko-kr/blogs/deep-learning/research/overview-interpretable-machine-learning-2-interpreting-deep-learning-models-image-recognition)

attentionì„ ì œëŒ€ë¡œ ì•Œì•„ë³´ê¸° ì „ì— ë¨¼ì € **feature selection**(íŠ¹ì§• ì„ íƒ)ì„ ë¨¼ì € ì‚´í´ë³´ì. attentionì˜ ì¼ì¢…ì€ ì•„ë‹ˆì§€ë§Œ ë§ˆì°¬ê°€ì§€ë¡œ ë„ì›€ì´ ë˜ëŠ” íŠ¹ì§•ë§Œ ê³¨ë¼ì„œ ì‚¬ìš©í•œë‹¤ëŠ” ì ì€ ê°™ë‹¤.

- ì“¸ëª¨ê°€ ë§ì€ feature(íŠ¹ì§•)ì€ ë‚¨ê¸´ë‹¤.

- ë‚˜ë¨¸ì§€ëŠ” ì œê±°í•œë‹¤.

4ê°€ì§€ feature( $x_1, x_2, x_3, x_4$ )ê°€ ìˆëŠ” ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.

![feature selection ex](images/feature_selection_ex.png)

> ë¹¨ê°„ìƒ‰: ë¶ˆëŸ‰ / íŒŒë€ìƒ‰: ì •ìƒ

ê°€ë ¹ ìœ„ ì˜ˆì‹œì—ì„œ feature 2ê°œë§Œ ì„ íƒí•œë‹¤ë©´(: 1), ë¶„ë³„ë ¥ì´ ê°•í•œ $x_{2}, x_{3}$ ì„ ì·¨í•  ê²ƒì´ë‹¤.(ì„ íƒí•˜ì§€ ì•ŠìŒ: 0)

---

### 11.1.2 ê³ ì „ ì•Œê³ ë¦¬ì¦˜: saliency map

**saliency map**(ëŒì¶œ ë§µ)ì€ attentioní•´ì•¼ í•˜ëŠ” ì •ë„ë¥¼ ì‹¤ìˆ˜ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤. pixel ê°’ì˜ ë³€í™”ê°€ ê¸‰ê²©í•œ ë¶€ë¶„ì„ ëª¨ì•„ì„œ mappingí•œ ë’¤ ë°°ê²½ê³¼ ë¶„ë¦¬í•œë‹¤.

![saliency map](images/saliency_map.jpg)

---

### 11.1.3 ë”¥ëŸ¬ë‹ì˜ attention

![CV attention milestone](images/CV_attention_milestone.png)

- RAM(Recurrent Attention Model): 2014ë…„ ë°œí‘œ. RNNì„ ì´ìš©í•´ ë”¥ëŸ¬ë‹ì—ì„œ ìµœì´ˆë¡œ attentionì„ ì ìš©í•œ ëª¨ë¸

- STN(Spatial Transformer Network): 2015ë…„ ë°œí‘œ. feature mapì— affine(translation, scaling, rotation) transformì„ ì ìš©í•´ì„œ attentioní•  ê³³ì„ ì •í•˜ëŠ” ëª¨ë¸

  - ë³€í™˜ í–‰ë ¬ì„ í•™ìŠµìœ¼ë¡œ ì•Œì•„ë‚¸ë‹¤.

- SENet(Squeeze-and-Excite Network): 2017ë…„ ë°œí‘œ. feature mapì˜ ì–´ëŠ channelì— attentioní• ì§€ ì•Œì•„ë‚¸ë‹¤.

    ![SEnet](images/SENet.png)

  > [SENet ì •ë¦¬: #7.2.8 SENet: Squeeze-and-Excitation block](https://github.com/erectbranch/TinyML_and_Efficient_DLC/tree/master/lec07/summary01)

---

### 11.1.4 ë”¥ëŸ¬ë‹ì˜ attention: self-attention

ì´ì „ê¹Œì§€ëŠ” 'ì¤‘ìš”í•œ ë¶€ë¶„ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì¤˜ì„œ ì„±ëŠ¥ì„ ê°œì„ 'í–ˆë‹¤.

- ì¤‘ìš”í•œ ê³³ì€ í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³ , ì¤‘ìš”í•˜ì§€ ì•Šì€ ê³³ì€ ì‘ì€ ê°€ì¤‘ì¹˜ë¥¼ ì¤€ë‹¤.

í•˜ì§€ë§Œ **self-attention**(ìê¸° ì£¼ëª©)ì€ ì˜ìƒì„ êµ¬ì„±í•˜ëŠ” ìš”ì†Œ ìƒí˜¸ ê°„ì˜ ê´€ê³„ë¥¼ ì°¾ì•„ë‚¸ë‹¤.

![attention vs self-attention](images/attention_vs_self_attention.png)

self-attentionì€ í”„ë¦¬ìŠ¤ë¹„ê°€ ë‹¤ë¥¸ ìœ„ì¹˜ì— attentioní•˜ëŠ” ì •ë„ë¥¼ ì„ ì˜ êµµê¸°ë¡œ í‘œí˜„í•œë‹¤.

- í”„ë¦¬ìŠ¤ë¹„ë¥¼ ì¡ìœ¼ë ¤ëŠ” ê°œì— ë§ì´ attentioní•œë‹¤.

- í•˜ëŠ˜ì—ëŠ” ë³„ë¡œ attentioní•˜ì§€ ì•ŠëŠ”ë‹¤.

> ì°¸ê³ ë¡œ 2018ë…„ [Non-local Neural Network](https://arxiv.org/abs/1711.07971)ì—ì„œ CVì— ì²˜ìŒ self-attentionì´ ì ìš©ë˜ì—ˆë‹¤. ì•„ë˜ ì‹ì„ í†µí•´ self-attentionì„ ê³„ì‚°í•œë‹¤.

$$ y_{i} = {{1} \over {C(\mathbf{x})}}{\sum}_{j}{a(\mathbf{x_{i}}, \mathbf{x_{j}})g(\mathbf{x_{j}})} $$

---

## 11.2 RNN

**RNN**(Recurrent Neural Network, ìˆœí™˜ ì‹ ê²½ë§)ì€ hidden nodeë¼ë¦¬ edgeë¥¼ ì´ì–´ì„œ ìˆœí™˜ êµ¬ì¡°ë¥¼ ë§Œë“ ë‹¤.

![RNN](images/RNN.png)

- weight ì§‘í•©: $\{U^{1}, U^{2}, U^{3}\}$ 

  - $U^{1}$ : input layer ~ **hidden layer**

  - $U^{2}$ : **hidden layer** ~ output layer

  - $U^{3}$ : **hidden layer** ~ **hidden layer**

ì´ì œ ë¬¸ì¥ì„ í‘œí˜„í•œ ë²¡í„° $\mathbf{o=(x^{1}, x^{2}, x^{3} , \cdots , x^{T})}$ ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤ê³  í•˜ì. ê¸¸ì´ëŠ” $T$ ì´ë©°, $i$ timeì˜ ë‹¨ì–´ $x^{i}$ ëŠ” ë²¡í„°ë‹¤.

> ì˜ˆë¥¼ ë“¤ì–´ 'ì €ê²Œ / ì €ì ˆë¡œ / ë¶‰ì–´ì§ˆ / ë¦¬ / ì—†ë‹¤'ëŠ” ë¬¸ì¥ì€ $T=5$ ì´ë‹¤. embeddingë˜ì–´ ë²¡í„°ë¡œ ì…ë ¥ì´ ë“¤ì–´ê°„ë‹¤.

ìœ„ êµ¬ì¡° ê·¸ë¦¼ì—ì„œ '(c) ì‹œê°„ì¶•ìœ¼ë¡œ í¼ì¹¨' ë¶€ë¶„ì„ ë³´ì.

- $\mathbf{x}^{3}, \mathbf{h}^{2}$ ê°€ ì…ë ¥ë˜ì–´ $\mathbf{h}^{3}$ ì´ ìƒì„±ëœë‹¤. ì¶œë ¥ìœ¼ë¡œ $\mathbf{o}^{3}$ ì´ ë‚˜ì˜¨ë‹¤.

- ì¼ë°˜í™”: $\mathbf{x}^{i}, \mathbf{h}^{i-1}$ ì´ ì…ë ¥ë˜ì–´ $\mathbf{h}^{i}$ ê°€ ìƒì„±ëœë‹¤. ì¶œë ¥ìœ¼ë¡œ $\mathbf{o}^{i}$ ê°€ ë‚˜ì˜¨ë‹¤.

ì¼ë°˜í™”ëŠ” time $i$ dì—ì„œ ì¶œë ¥ì´ ë‚˜ì˜¤ê¸°ê¹Œì§€ì˜ ë™ì‘ì„ ì •ì˜í•œ ê²ƒì´ë‹¤. ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ì.

- ì…ë ¥ $\mathbf{x}^i$ ì— weight $\mathbf{U}^1$ ì„ ê³±í•œ ê²°ê³¼ì™€, ì…ë ¥ $\mathbf{h}^{i-1}$ ì— $\mathbf{U}^3$ ì„ ê³±í•œ ê²°ê³¼ë¥¼ ë”í•´ì„œ, time $i$ ì˜ hidden vector $\mathbf{h}^{i}$ ë¥¼ ë§Œë“ ë‹¤.

$$ \mathbf{h}^i = {\tau}_{1}(\mathbf{U}^3 \mathbf{h}^{i-1} + \mathbf{U}^1 \mathbf{x}^i) $$

- $\mathbf{h}^{i}$ ì— $\mathbf{U}^2$ ë¥¼ ê³±í•˜ê³  activation function ${\tau}_{2}$ ë¥¼ ê³±í•´ì„œ, output $\mathbf{o}^i$ ë¥¼ ì¶œë ¥í•œë‹¤.

$$ \mathbf{o}^i = {\tau}_{2}(\mathbf{U}^2 \mathbf{h}^i) $$

ì´ì²˜ëŸ¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ì‚°ì´ ì§„í–‰ë˜ê³ , ì§€ë‚œ hidden stateë¡œ ë‹¤ìŒ hidden stateë¥¼ ë§Œë“¤ê¸° ë•Œë¬¸ì— ê³„ì† ì´ì „ ì •ë³´ì™€ ìƒí˜¸ì‘ìš©í•˜ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì„¸ ë²ˆì§¸ ë‹¨ì–´ë¼ë©´ ì²« ë²ˆì§¸ ë‹¨ì–´ì™€ ë‘ ë²ˆì§¸ ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ì™€ ìƒí˜¸ì‘ìš©í•œë‹¤.

---

### 11.2.1 LSTM

ê¸°ì¡´ RNNì€ $1, 2, ..., i$ ë¡œ ê°€ë©° ì˜¤ë˜ëœ ë‹¨ì–´ì˜ ì •ë³´ê°€ í¬ë¯¸í•´ì§€ëŠ”ë°, ì¢…ì¢… ì•ìª½ ë‹¨ì–´ì™€ ë©€ë¦¬ ë’¤ì— ìˆëŠ” ë‹¨ì–´ê°€ ë°€ì ‘í•˜ê²Œ ìƒí˜¸ì‘ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤.(**long-range dependency**)

**LSTM**(Long Short-Term Memory, ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬)ëŠ” RNNì˜ í•œ ì¢…ë¥˜ë¡œ, ê³³ê³³ì— input, outputì„ ì—´ê±°ë‚˜ ë§‰ì„ ìˆ˜ ìˆëŠ” gateë¥¼ ë‘ì–´ì„œ ì„ ë³„ì ìœ¼ë¡œ ê¸°ì–µí•˜ëŠ” ê¸°ëŠ¥ì„ í™•ë³´í•œ ëª¨ë¸ì´ë‹¤.

![LSTM](images/LSTM.png)

- ì—¬ë‹«ëŠ” ì •ë„ëŠ” í•™ìŠµìœ¼ë¡œ ì•Œì•„ë‚¸ weightì— ë”°ë¼ ê²°ì •ëœë‹¤.

---

### 11.2.2 seq2seq

seq2seqëŠ” 2014ë…„ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë°œí‘œëœ íšê¸°ì ì¸ ëª¨ë¸ì´ë‹¤. 

- ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ì„ ë˜ ë‹¤ë¥¸ ê°€ë³€ ê¸¸ì´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

   > ì˜ˆë¥¼ ë“¤ì–´ ë¬¸ì¥ ë²ˆì—­ ì‹œ seq2seqë¥¼ ì´ìš©í•´ì„œ, ê¸¸ì´ê°€ ë‹¤ë¥¸ ëª©í‘œ ì–¸ì–´ì˜ ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

encoder, decoderë¡œ êµ¬ì„±ëœ seq2seq ëª¨ë¸ì˜ í•™ìŠµ ë‹¨ê³„, ì¶”ë¡  ë‹¨ê³„ ë™ì‘ ë°©ì‹ì„ ì‚´í´ë³´ì.

1. ìš°ì„  í•™ìŠµ ë‹¨ê³„ëŠ” decoderì˜ input, outputì´ ëª¨ë‘ ë™ì‘í•œë‹¤.

    ![seq2seq train](images/seq2seq_training.png)

    - ë‹¤ì‹œ ë§í•´ ì •ë‹µ ë¬¸ì¥ì„ decoderì˜ inputìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” **teacher forcing**(êµì‚¬ ê°•ìš”) ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

<br/>

2. ì¶”ë¡ (ì˜ˆì¸¡) ë‹¨ê³„ì—ì„œëŠ” decoderì˜ inputì„ ë¹¼ê³ , **auto-regressive**(ìê¸° íšŒê·€) ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤.

    ![seq2seq inference](images/seq2seq_inference.png)

    - ë‹¤ì‹œ ë§í•´ decoderì˜ outputì„ ë‹¤ìŒ inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

        > time 1ì—ì„œ \<SOS\>ê°€ ì…ë ¥ë˜ë©´, ì²« ë‹¨ì–´ 'That'ì„ ì¶œë ¥í•˜ê³ , ì¶œë ¥ 'That'ì„ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•´ì„œ time 2ì—ì„œ 'can't'ë¥¼ ì¶œë ¥í•œë‹¤. ì´ëŸ° ì‹ìœ¼ë¡œ \<EOS\>ê°€ ì¶œë ¥ë  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.

auto-regressive ë°©ì‹ì„ ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ \mathbf{g}^{i} = {\tau}_{i} (\mathbf{U}^{3}\mathbf{g}^{i-1} + \mathbf{U}^{1}\mathbf{y}^{i-1}) $$

$$ \mathbf{o}^{i} = {\tau}_{2}(\mathbf{U}^{2}\mathbf{g}^{i}) $$

ê·¸ëŸ°ë° ì´ëŸ¬í•œ seq2seqë„ í•œê³„ë¥¼ ê°€ì§„ë‹¤. ì œì¼ í° ë¬¸ì œëŠ” **encoderì˜ ë§ˆì§€ë§‰ hidden stateë§Œ decoderë¡œ ì „ë‹¬ëœë‹¤**ëŠ” ì ì´ë‹¤.

- ë”°ë¼ì„œ encoderëŠ” ë§ˆì§€ë§‰ hidden stateì— ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•í•´ì•¼ í•œë‹¤.

- decoderëŠ” encoderì˜ ëª¨ë“  ìˆœê°„ì— ìˆëŠ” í’ë¶€í•œ ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ì—†ë‹¤.

---

### 11.2.3 query-key-value

seq2seqì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ í•œ ë°©ë²•ìœ¼ë¡œ, **query-key-value**ìœ¼ë¡œ attentionì„ ì ìš©í•œ ì—°êµ¬ê°€ ìˆë‹¤. ìš°ì„  query, key, valueê°€ ë¬´ì—‡ì¸ì§€ ê°œë…ë¶€í„° ì•Œì•„ë³´ì. 

> keyì™€ valueëŠ” Pythonì˜ ë”•ì…”ë„ˆë¦¬ ìë£Œí˜•ì„ ìƒê°í•´ ë³´ë©´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤.

```Python
dict = {"2017": "Transformer", "2018": "BERT"}
```

attentionì€ ë‹¤ìŒ ê³¼ì •ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

- attention functionì€ ì£¼ì–´ì§„ queryë¥¼ ê°€ì§€ê³  ëª¨ë“  keyì—ì„œ ìœ ì‚¬ë„ë¥¼ êµ¬í•œë‹¤. ê·¸ ë‹¤ìŒ ìœ ì‚¬ë„ë¥¼ ê° keyì˜ valueì— ë°˜ì˜í•œë‹¤.

  - ë‹¤ì‹œ ë§í•´ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•´ì„œ valueì— ê°€ì¤‘í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” ê²ƒì´ë‹¤.

    ![query, key, value](images/query_key_value.png)

- ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ valueë¥¼ ëª¨ë‘ ë”í•œ í›„ ë°˜í™˜ëœ ê°’ì„ **attention value**ë¼ê³  í•œë‹¤. = $Attention(\mathbf{q}, \mathbf{K}, \mathbf{V})$ 

ì‹¤ì œ ì˜ˆì‹œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚´í´ë³´ì.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: query-key-valueë¥¼ ì´ìš©í•œ attention ê³„ì‚°&nbsp;&nbsp;&nbsp;</span>

ìœ ì‚¬ë„ $s_{i}$ ëŠ” inner product $\mathbf{q} {\mathbf{k}_{i}}^{T}$ ë°©ì‹ìœ¼ë¡œ ì‚°ì •í•œë‹¤ê³  ê°€ì •í•œë‹¤.

- query $\mathbf{q}$ = (0.990, 0.099, 0.099)

- key, valueëŠ” 4ê°œ: $k_{1}, \cdots k_{4}$ , $v_{1}, \cdots , v_{4}$

$$ \mathbf{K} = \begin{bmatrix} 0.050 & 0.000 & 0.998 \\ 0.020 & 0.020 & 0.999 \\ 0.976 & 0.098 & 0.195 \\ 0.020 & 0.999 & 0.020 \end{bmatrix} $$

$$ \mathbf{V} = \begin{bmatrix} 1 & 2 & 5 \\ 1 & 1 & 5 \\ 3 & 2 & 4 \\ 6 & 1 & 2 \end{bmatrix} $$

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

![query-key-value ì˜ˆì œ ì •ë‹µ](images/query-key-value_example_answer.png)

ìš°ì„  **attention vector** $\mathbf{a}$ ë¥¼ êµ¬í•˜ì. ìœ ì‚¬ë„ ë²¡í„°ë¥¼ ê³„ì‚°í•œ ë’¤ softmaxë¥¼ ì ìš©í•œ ê²ƒì´ë‹¤.

$$ \mathbf{a} = \mathrm{softmax}(\mathbf{q}{\mathbf{K}}^{T}) = \mathrm{softmax} \left( \begin{bmatrix} 0.990 & 0.099 & 0.099 \end{bmatrix} \begin{bmatrix} 0.050 & 0.020 & 0.976 & 0.020 \\ 0.000 & 0.020 & 0.098 & 0.999 \\ 0.998 & 0.999 & 0.195 & 0.020 \end{bmatrix} \right) $$

$$ = \begin{bmatrix} 0.189 & 0.184 & 0.442 & 0.184 \end{bmatrix} $$

ê·¸ ë‹¤ìŒì€ **contect vector** $\mathbf{c}$ ë¥¼ ê³„ì‚°í•œë‹¤. attention vectorì™€ valueë¥¼ ê³±í•œ í›„ ëª¨ë‘ í•©ì‚°í•œ ê°’ì´ë‹¤.

$$ \mathbf{c} = \mathrm{softmax}(\mathbf{q}{\mathbf{K}}^{T})\mathbf{V} = \begin{bmatrix} 0.189 & 0.184 & 0.442 & 0.184 \end{bmatrix} \begin{bmatrix} 1 & 2 & 5 \\ 1 & 1 & 5 \\ 3 & 2 & 4 \\ 6 & 1 & 2 \end{bmatrix} $$

$$ = \begin{bmatrix} 2.805 & 1.631 & 4.005 \end{bmatrix} $$

---

### 11.2.4 attentionì„ ë°˜ì˜í•œ seq2seq ëª¨ë¸

ì§€ë‚œ ê·¸ë¦¼ì„ ë‹¤ì‹œ ì‚´í´ë³´ì.

![seq2seq inference](images/seq2seq_inference.png)

- ì˜ˆë¥¼ ë“¤ì–´ time 6ì—ì„œ decoderëŠ” 'ì €ì ˆë¡œ'ì— ì£¼ëª©í•´ì•¼ 'itself'ë¥¼ ì œëŒ€ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.

- ê·¸ë ‡ë‹¤ë©´ $\begin{bmatrix} 0.01 & 0.9 & 0.02 & 0.03 & 0.04 \end{bmatrix}$ ì²˜ëŸ¼ ë‘ ë²ˆì§¸ ìš”ì†Œ('ì €ì ˆë¡œ') ê°’ì´ í° attention vector $\mathbf{a}$ ë¥¼ ìƒì„±í•˜ë©´, decoderëŠ” 'ì €ì ˆë¡œ'ì— ì£¼ëª©í•  ìˆ˜ ìˆê²Œ ëœë‹¤.

ì´ì œ ì´ëŸ¬í•œ ìƒí™©ì—ì„œ query-key-valueë¡œ ë¬´ì—‡ì„ ì‚¬ìš©í• ì§€ ì •í•´ ë³´ì.

- time 6ëŠ” time 5ì—ì„œ ${\mathbf{g}}^{5}$ ë¥¼ ë°›ëŠ”ë‹¤. ì´ê²ƒì„ queryë¡œ í™œìš©í•  ê²ƒì´ë‹¤.

  - ì˜ˆì‹œë¡œ ${\mathbf{g}}^{5}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•œë‹¤.

$$\mathbf{q} = \begin{bmatrix} 0.2 & 0.9 & 0.0 \end{bmatrix}$$

- key, valueëŠ” ì–´ë–¤ ê°’ì„ ì‚¬ìš©í• ê¹Œ? ë°”ë¡œ ${\mathbf{h}}^{1}, {\mathbf{h}}^{2}, \cdots , {\mathbf{h}}^{5}$ ë¥¼ ì‚¬ìš©í•œë‹¤.

   - ì˜ˆì‹œë¡œ ${\mathbf{h}}^{1}, {\mathbf{h}}^{2}, \cdots , {\mathbf{h}}^{5}$ ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•œë‹¤.

$$ \mathbf{K} = \begin{bmatrix} 0.1 & 0.0 & 0.8 \\ 0.1 & 0.9 & 0.0 \\ 0.0 & 0.1 & 0.8 \\ 0.2 & 0.1 & 0.6 \\ 0.9 & 0.0 & 0.1 \end{bmatrix} $$

$$ \mathbf{V} = \begin{bmatrix} 0.1 & 0.0 & 0.8 \\ 0.1 & 0.9 & 0.0 \\ 0.0 & 0.1 & 0.8 \\ 0.2 & 0.1 & 0.6 \\ 0.9 & 0.0 & 0.1 \end{bmatrix} $$

ì§€ë‚œ ìœ ì‚¬ë„ë¥¼ êµ¬í•œ ë’¤ softmaxë¥¼ ì ìš©í•˜ì—¬ attention vectorë¥¼ êµ¬í•œë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ valueì™€ ê³±í•œ í›„ ëª¨ë‘ í•©ì‚°í•˜ë©´ contect vector $\mathbf{c} = \begin{bmatrix} 0.243 & 0.339 & 0.370 \end{bmatrix}$ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

ì´ë ‡ê²Œ êµ¬í•´ë‚¸ contect vectorëŠ” decoderì˜ ì…ë ¥ í•­ì˜ í•œ ìë¦¬ë¥¼ ì°¨ì§€í•˜ê²Œ ëœë‹¤. ê°€ì¤‘ì¹˜ ${\mathbf{U}}^{4}$ ë¥¼ ê³±í•œ ë’¤ ì¶”ê°€ë˜ë©°, ì´ ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµì„ í†µí•´ì„œ ì¶”ê°€ë¡œ ì•Œì•„ë‚´ì•¼ í•œë‹¤.

![Bahdanau attention seq2seq](images/Bahdanau_attention_seq2seq.png)

$$ {\mathbf{g}}^{i} = {\tau}_{1}({\mathbf{U}}^{3}{\mathbf{g}}^{i-1} + {\mathbf{U}}^{1}{\mathbf{y}}^{i-1} + {\mathbf{U}}^{4}{\mathbf{c}}^{i}) $$


$$ {\mathbf{o}}^{i} = {\tau}_{2}({\mathbf{U}}^{2}{\mathbf{g}}^{i}) $$

---